J. Devlin, M. W. Chang, K. Lee, and K. Toutanova, "BERT: Pre-training of Deep Bidirectional Transformers for 
Language Understanding," arXiv preprint arXiv:1810.04805, 2018.
A. Vaswani et al., "Attention is all you need," in Advances in Neural Information Processing Systems, pp. 5998 -6008, 
2017.
T. B. Brown et al., "Language Models are Few-Shot Learners," arXiv preprint arXiv:2005.14165, 2020.
C. Raffel et al., "Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer," Journal of Machine 
Learning Research, vol. 21, no. 140, pp. 1-67, 2020.
Y. Liu et al., "RoBERTa: A Robustly Optimized BERT Pretraining Approach," arXiv preprint arXiv:1907.11692, 2019.
Y. Zhang and B. Liu, "Deep Learning for Sentiment Analysis: A Survey," Wiley Encyclopedia of Operations Research 
and Management Science, 2018.
Hugging Face, "BERT Tokenizer Documentation," 2024. Available: 
https://huggingface.co/docs/transformers/model_doc/bert
M. Lewis et al., "BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, 
and Comprehension," arXiv preprint arXiv:1910.13461, 2020.
D. P. Kingma and J. Ba, "Adam: A Method for Stochastic Optimization," arXiv preprint arXiv:1412.6980, 2015.
J. Brownlee, "Classification Cross-Entropy Loss Function for Machine Learning," Machine Learning Mastery, 2020.
Y. Bengio, "Practical Recommendations for Gradient-Based Training of Deep Architectures," in Neural Networks: 
Tricks of the Trade, 2012
